# RAGFlow ë°ì´í„°ì…‹(ì§€ì‹ë² ì´ìŠ¤) êµ¬ì¶• ì‹œìŠ¤í…œ ìƒì„¸ ë¶„ì„

## ğŸ“‹ ëª©ì°¨
1. [ë°ì´í„° ëª¨ë¸ & ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ](#1-ë°ì´í„°-ëª¨ë¸--ë°ì´í„°ë² ì´ìŠ¤-ìŠ¤í‚¤ë§ˆ)
2. [ì§€ì‹ë² ì´ìŠ¤ ìƒì„± & ê´€ë¦¬ API](#2-ì§€ì‹ë² ì´ìŠ¤-ìƒì„±--ê´€ë¦¬-api)
3. [ë¬¸ì„œ ì—…ë¡œë“œ & ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸](#3-ë¬¸ì„œ-ì—…ë¡œë“œ--ì²˜ë¦¬-íŒŒì´í”„ë¼ì¸)
4. [ì²­í‚¹(Chunking) ì „ëµ & í…ìŠ¤íŠ¸ ì¶”ì¶œ](#4-ì²­í‚¹chunking-ì „ëµ--í…ìŠ¤íŠ¸-ì¶”ì¶œ)
5. [ì„ë² ë”© ìƒì„± & ì¸ë±ì‹±](#5-ì„ë² ë”©-ìƒì„±--ì¸ë±ì‹±)
6. [í”„ë¡ íŠ¸ì—”ë“œ UI ì»´í¬ë„ŒíŠ¸](#6-í”„ë¡ íŠ¸ì—”ë“œ-ui-ì»´í¬ë„ŒíŠ¸)
7. [ì „ì²´ ì›Œí¬í”Œë¡œìš° ìš”ì•½](#7-ì „ì²´-ì›Œí¬í”Œë¡œìš°-ìš”ì•½)

---

## 1. ë°ì´í„° ëª¨ë¸ & ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

### í•µì‹¬ ë°ì´í„° ëª¨ë¸ (Peewee ORM)

#### **Knowledgebase ëª¨ë¸** ([api/db/db_models.py:736-771](api/db/db_models.py#L736-L771))

```python
class Knowledgebase(DataBaseModel):
    # ê¸°ë³¸ ì •ë³´
    id = CharField(max_length=32, primary_key=True)  # UUID
    avatar = TextField(null=True)  # Base64 ì•„ë°”íƒ€
    tenant_id = CharField(max_length=32)  # í…Œë„ŒíŠ¸(ì‚¬ìš©ì) ID
    name = CharField(max_length=128)  # KB ì´ë¦„
    language = CharField(max_length=32, default="English|Chinese")
    description = TextField(null=True)

    # ëª¨ë¸ ì„¤ì •
    embd_id = CharField(max_length=128)  # ì„ë² ë”© ëª¨ë¸ ID
    parser_id = CharField(max_length=32, default=ParserType.NAIVE.value)  # íŒŒì„œ íƒ€ì…
    pipeline_id = CharField(max_length=32, null=True)  # íŒŒì´í”„ë¼ì¸ ID
    parser_config = JSONField(default={"pages": [[1, 1000000]]})  # íŒŒì„œ ì„¤ì •

    # í†µê³„ ì •ë³´
    doc_num = IntegerField(default=0)  # ë¬¸ì„œ ìˆ˜
    token_num = IntegerField(default=0)  # ì´ í† í° ìˆ˜
    chunk_num = IntegerField(default=0)  # ì´ ì²­í¬ ìˆ˜

    # ê²€ìƒ‰ ì„¤ì •
    similarity_threshold = FloatField(default=0.2)  # ìœ ì‚¬ë„ ì„ê³„ê°’
    vector_similarity_weight = FloatField(default=0.3)  # ë²¡í„° ê°€ì¤‘ì¹˜
    pagerank = IntegerField(default=0)  # PageRank ì ìˆ˜

    # ê³ ê¸‰ ê¸°ëŠ¥ íƒœìŠ¤í¬
    graphrag_task_id = CharField(max_length=32, null=True)  # GraphRAG íƒœìŠ¤í¬
    graphrag_task_finish_at = DateTimeField(null=True)
    raptor_task_id = CharField(max_length=32, null=True)  # RAPTOR íƒœìŠ¤í¬
    raptor_task_finish_at = DateTimeField(null=True)
    mindmap_task_id = CharField(max_length=32, null=True)  # Mindmap íƒœìŠ¤í¬
    mindmap_task_finish_at = DateTimeField(null=True)

    # ê¶Œí•œ & ìƒíƒœ
    permission = CharField(max_length=16, default="me")  # me|team
    created_by = CharField(max_length=32)
    status = CharField(max_length=1, default="1")  # 0: ì‚­ì œë¨, 1: ìœ íš¨
```

#### **Document ëª¨ë¸** ([api/db/db_models.py:773-799](api/db/db_models.py#L773-L799))

```python
class Document(DataBaseModel):
    id = CharField(max_length=32, primary_key=True)
    thumbnail = TextField(null=True)  # ì¸ë„¤ì¼ Base64
    kb_id = CharField(max_length=256)  # ì†Œì† KB ID

    # íŒŒì„œ ì„¤ì •
    parser_id = CharField(max_length=32)
    pipeline_id = CharField(max_length=32, null=True)
    parser_config = JSONField(default={"pages": [[1, 1000000]]})

    # íŒŒì¼ ì •ë³´
    source_type = CharField(max_length=128, default="local")  # local|web|...
    type = CharField(max_length=32)  # íŒŒì¼ í™•ì¥ì (pdf, docx ë“±)
    suffix = CharField(max_length=32)  # ì‹¤ì œ íŒŒì¼ í™•ì¥ì
    name = CharField(max_length=255)  # íŒŒì¼ ì´ë¦„
    location = CharField(max_length=255)  # ì €ì¥ ìœ„ì¹˜
    size = IntegerField(default=0)  # íŒŒì¼ í¬ê¸°

    # ì²˜ë¦¬ í†µê³„
    token_num = IntegerField(default=0)
    chunk_num = IntegerField(default=0)
    progress = FloatField(default=0)  # 0.0 ~ 1.0
    progress_msg = TextField(null=True)
    process_begin_at = DateTimeField(null=True)
    process_duration = FloatField(default=0)
    meta_fields = JSONField(null=True, default={})

    # ì‹¤í–‰ ìƒíƒœ
    run = CharField(max_length=1, default="0")  # 0: ëŒ€ê¸°, 1: ì‹¤í–‰, 2: ì·¨ì†Œ
    status = CharField(max_length=1, default="1")  # ìœ íš¨ì„±
    created_by = CharField(max_length=32)
```

#### **File & File2Document ëª¨ë¸**

```python
class File(DataBaseModel):
    id = CharField(max_length=32, primary_key=True)
    parent_id = CharField(max_length=32)  # í´ë” êµ¬ì¡°
    tenant_id = CharField(max_length=32)
    name = CharField(max_length=255)
    location = CharField(max_length=255)
    size = IntegerField(default=0)
    type = CharField(max_length=32)  # file|folder
    source_type = CharField(max_length=128)

class File2Document(DataBaseModel):
    # Fileê³¼ Documentì˜ ë‹¤ëŒ€ë‹¤ ê´€ê³„
    file_id = CharField(max_length=32)
    document_id = CharField(max_length=32)
```

### ë°ì´í„°ë² ì´ìŠ¤ ê´€ê³„ë„

```
Tenant (User)
    â†“ 1:N
Knowledgebase
    â†“ 1:N
Document â†â†’ File (N:M via File2Document)
    â†“ 1:N
Chunk (Elasticsearch/Infinityì— ì €ì¥)
```

---

## 2. ì§€ì‹ë² ì´ìŠ¤ ìƒì„± & ê´€ë¦¬ API

### 2.1 KB ìƒì„± API

**ì—”ë“œí¬ì¸íŠ¸**: `POST /v1/dataset/create` ([api/apps/kb_app.py:43-106](api/apps/kb_app.py#L43-L106))

**ìš”ì²­ ë³¸ë¬¸**:
```json
{
  "name": "My Knowledge Base",
  "description": "Optional description",
  "parser_id": "naive",
  "embd_id": "BAAI/bge-large-zh-v1.5"
}
```

**ì²˜ë¦¬ ë¡œì§**:

```python
def create():
    req = request.json
    dataset_name = req["name"]

    # 1. ì´ë¦„ ìœ íš¨ì„± ê²€ì‚¬
    if len(dataset_name.encode("utf-8")) > DATASET_NAME_LIMIT:
        return error

    # 2. ì¤‘ë³µ ì´ë¦„ ì²˜ë¦¬ (ìë™ìœ¼ë¡œ "(1)" ì¶”ê°€)
    dataset_name = duplicate_name(
        KnowledgebaseService.query,
        name=dataset_name,
        tenant_id=current_user.id
    )

    # 3. ê¸°ë³¸ íŒŒì„œ ì„¤ì • ìƒì„±
    req["parser_config"] = {
        "layout_recognize": "DeepDOC",  # Vision ëª¨ë¸ ì‚¬ìš©
        "chunk_token_num": 512,  # ì²­í¬ë‹¹ í† í° ìˆ˜
        "delimiter": "\n",
        "auto_keywords": 0,  # LLM í‚¤ì›Œë“œ ì¶”ì¶œ ë¹„í™œì„±í™”
        "auto_questions": 0,  # LLM ì§ˆë¬¸ ìƒì„± ë¹„í™œì„±í™”
        "html4excel": False,
        "topn_tags": 3,
        "raptor": {
            "use_raptor": True,
            "prompt": "...",
            "max_token": 256,
            "threshold": 0.1,
            "max_cluster": 64
        },
        "graphrag": {
            "use_graphrag": True,
            "entity_types": ["organization", "person", "geo", "event", "category"],
            "method": "light"
        }
    }

    # 4. UUID ìƒì„± ë° ì €ì¥
    req["id"] = get_uuid()
    req["tenant_id"] = current_user.id
    KnowledgebaseService.save(**req)

    return {"kb_id": req["id"]}
```

**ê¸°ë³¸ íŒŒì„œ ì„¤ì • ìƒì„¸**:

| ì„¤ì • | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|--------|------|
| `layout_recognize` | "DeepDOC" | Vision ëª¨ë¸ë¡œ ë ˆì´ì•„ì›ƒ ë¶„ì„ |
| `chunk_token_num` | 512 | ì²­í¬ë‹¹ ìµœëŒ€ í† í° ìˆ˜ |
| `delimiter` | "\n" | í…ìŠ¤íŠ¸ ë¶„í•  êµ¬ë¶„ì |
| `auto_keywords` | 0 | LLM í‚¤ì›Œë“œ ì¶”ì¶œ ê°œìˆ˜ (0=ë¹„í™œì„±í™”) |
| `auto_questions` | 0 | LLM ì§ˆë¬¸ ìƒì„± ê°œìˆ˜ (0=ë¹„í™œì„±í™”) |
| `raptor.use_raptor` | True | RAPTOR ì•Œê³ ë¦¬ì¦˜ í™œì„±í™” |
| `graphrag.use_graphrag` | True | GraphRAG ì—”í‹°í‹° ì¶”ì¶œ í™œì„±í™” |

### 2.2 KB ì—…ë°ì´íŠ¸ API

**ì—”ë“œí¬ì¸íŠ¸**: `POST /v1/dataset/update` ([api/apps/kb_app.py:109-170](api/apps/kb_app.py#L109-L170))

**ì£¼ìš” ê¸°ëŠ¥**:
- KB ì´ë¦„, ì„¤ëª…, íŒŒì„œ ì„¤ì • ë³€ê²½
- `pagerank` ê°’ ë³€ê²½ ì‹œ Elasticsearch ì¸ë±ìŠ¤ ìë™ ì—…ë°ì´íŠ¸
- ì†Œìœ ìë§Œ ìˆ˜ì • ê°€ëŠ¥ (ê¶Œí•œ ê²€ì¦)

### 2.3 KB ìƒì„¸ ì¡°íšŒ API

**ì—”ë“œí¬ì¸íŠ¸**: `GET /v1/dataset/detail?kb_id=xxx` ([api/apps/kb_app.py:173-197](api/apps/kb_app.py#L173-L197))

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "id": "kb_uuid",
  "name": "My KB",
  "description": "...",
  "doc_num": 15,
  "chunk_num": 3420,
  "token_num": 850000,
  "size": 52428800,
  "parser_id": "naive",
  "embd_id": "BAAI/bge-large-zh-v1.5",
  "parser_config": {...},
  "graphrag_task_finish_at": "2025-01-15 10:30:00",
  "raptor_task_finish_at": "2025-01-15 11:45:00"
}
```

### 2.4 KB ì‚­ì œ API

**ì—”ë“œí¬ì¸íŠ¸**: `POST /v1/dataset/rm` ([api/apps/kb_app.py:235-274](api/apps/kb_app.py#L235-L274))

**ì‚­ì œ í”„ë¡œì„¸ìŠ¤**:
```python
def rm():
    kb_id = request.json["kb_id"]

    # 1. ê¶Œí•œ ê²€ì¦ (ì†Œìœ ìë§Œ ì‚­ì œ ê°€ëŠ¥)
    if not KnowledgebaseService.accessible4deletion(kb_id, current_user.id):
        return error

    # 2. ëª¨ë“  ë¬¸ì„œ ì‚­ì œ
    for doc in DocumentService.query(kb_id=kb_id):
        DocumentService.remove_document(doc, kb.tenant_id)
        # File2Document ê´€ê³„ ì‚­ì œ
        File2DocumentService.delete_by_document_id(doc.id)
        # MinIOì—ì„œ íŒŒì¼ ì‚­ì œ
        FileService.filter_delete([File.id == file_id])

    # 3. KB ìì²´ ì‚­ì œ
    KnowledgebaseService.delete_by_id(kb_id)

    # 4. Elasticsearch ì¸ë±ìŠ¤ ì‚­ì œ
    settings.docStoreConn.delete({"kb_id": kb_id}, index_name, kb_id)
    settings.docStoreConn.deleteIdx(index_name, kb_id)

    # 5. MinIO ë²„í‚· ì‚­ì œ (ìŠ¤í† ë¦¬ì§€ ì§€ì› ì‹œ)
    STORAGE_IMPL.remove_bucket(kb_id)
```

### 2.5 ê³ ê¸‰ ê¸°ëŠ¥ API

#### **GraphRAG ì‹¤í–‰** ([api/apps/kb_app.py:544-588](api/apps/kb_app.py#L544-L588))
```python
POST /v1/dataset/run_graphrag
{
  "kb_id": "xxx"
}
```
â†’ ì—”í‹°í‹° ì¶”ì¶œ ë° ì§€ì‹ ê·¸ë˜í”„ ìƒì„± íƒœìŠ¤í¬ íì‰

#### **RAPTOR ì‹¤í–‰** ([api/apps/kb_app.py:613-657](api/apps/kb_app.py#L613-L657))
```python
POST /v1/dataset/run_raptor
{
  "kb_id": "xxx"
}
```
â†’ ê³„ì¸µì  ìš”ì•½ íŠ¸ë¦¬ ìƒì„± íƒœìŠ¤í¬ íì‰

#### **ì§€ì‹ ê·¸ë˜í”„ ì¡°íšŒ** ([api/apps/kb_app.py:352-389](api/apps/kb_app.py#L352-L389))
```python
GET /v1/dataset/<kb_id>/knowledge_graph
```
â†’ GraphRAG ê²°ê³¼ ë…¸ë“œ/ì—£ì§€ ë°˜í™˜ (ìµœëŒ€ 256 ë…¸ë“œ, 128 ì—£ì§€)

---

## 3. ë¬¸ì„œ ì—…ë¡œë“œ & ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 3.1 íŒŒì¼ ì—…ë¡œë“œ API

**ì—”ë“œí¬ì¸íŠ¸**: `POST /v1/document/upload` ([api/apps/document_app.py:52-83](api/apps/document_app.py#L52-L83))

**ì—…ë¡œë“œ í”„ë¡œì„¸ìŠ¤**:

```python
def upload():
    kb_id = request.form.get("kb_id")
    file_objs = request.files.getlist("file")

    # 1. íŒŒì¼ëª… ìœ íš¨ì„± ê²€ì‚¬ (UTF-8 ê¸°ì¤€ ìµœëŒ€ ë°”ì´íŠ¸ ìˆ˜)
    for file_obj in file_objs:
        if len(file_obj.filename.encode("utf-8")) > FILE_NAME_LEN_LIMIT:
            return error

    # 2. KB ì¡´ì¬ ë° ê¶Œí•œ í™•ì¸
    kb = KnowledgebaseService.get_by_id(kb_id)
    if not check_kb_team_permission(kb, current_user.id):
        return error

    # 3. íŒŒì¼ ì—…ë¡œë“œ (MinIO)
    err, files = FileService.upload_document(kb, file_objs, current_user.id)

    return files
```

**FileService.upload_document ë¡œì§**:
```python
# 1. MinIOì— íŒŒì¼ ì €ì¥
location = f"{kb_id}/{file_uuid}"
STORAGE_IMPL.put(bucket=kb_id, name=location, binary=file_data)

# 2. File ë ˆì½”ë“œ ìƒì„±
file_record = {
    "id": file_uuid,
    "parent_id": kb_id,
    "tenant_id": tenant_id,
    "name": filename,
    "location": location,
    "size": file_size,
    "type": file_extension,
    "source_type": FileSource.KNOWLEDGEBASE
}
FileService.save(**file_record)

# 3. Document ë ˆì½”ë“œ ìƒì„±
doc_record = {
    "id": doc_uuid,
    "kb_id": kb_id,
    "parser_id": kb.parser_id,  # KBì˜ ê¸°ë³¸ íŒŒì„œ
    "parser_config": kb.parser_config,
    "name": filename,
    "type": file_extension,
    "size": file_size,
    "location": location,
    "run": TaskStatus.UNSTART  # íŒŒì‹± ëŒ€ê¸° ìƒíƒœ
}
DocumentService.save(**doc_record)

# 4. File-Document ê´€ê³„ ìƒì„±
File2DocumentService.save(file_id=file_uuid, document_id=doc_uuid)
```

### 3.2 ë¬¸ì„œ íŒŒì‹± íƒœìŠ¤í¬ íì‰

**ì—”ë“œí¬ì¸íŠ¸**: `POST /v1/document/run` (ë¬¸ì„œ íŒŒì‹± ì‹œì‘)

**íƒœìŠ¤í¬ ìƒì„± ë¡œì§** ([api/db/services/document_service.py](api/db/services/document_service.py)):

```python
def queue_tasks(doc):
    task = {
        "id": get_uuid(),
        "doc_id": doc.id,
        "kb_id": doc.kb_id,
        "tenant_id": tenant_id,
        "parser_id": doc.parser_id,
        "parser_config": doc.parser_config,
        "embd_id": kb.embd_id,
        "llm_id": llm_id,
        "language": kb.language,
        "name": doc.name,
        "location": doc.location,
        "size": doc.size,
        "from_page": 0,
        "to_page": 100000,
        "task_type": "dataflow",
        "pagerank": kb.pagerank
    }

    # Redis Streamì— íƒœìŠ¤í¬ ì¶”ê°€
    queue_name = get_svr_queue_name()
    REDIS_CONN.xadd(queue_name, {"message": json.dumps(task)})

    # Task ë ˆì½”ë“œ ìƒì„±
    TaskService.save(**task)
```

### 3.3 íƒœìŠ¤í¬ ì‹¤í–‰ì (Worker)

**ì‹¤í–‰ íŒŒì¼**: [rag/svr/task_executor.py](rag/svr/task_executor.py)

**ì›Œì»¤ ì•„í‚¤í…ì²˜**:
```python
# ë™ì‹œ ì‹¤í–‰ ì œí•œ
MAX_CONCURRENT_TASKS = 5  # ìµœëŒ€ ë™ì‹œ íƒœìŠ¤í¬
MAX_CONCURRENT_CHUNK_BUILDERS = 1  # ì²­í‚¹ ì‘ì—…
MAX_CONCURRENT_MINIO = 10  # MinIO ì‘ì—…

task_limiter = trio.Semaphore(MAX_CONCURRENT_TASKS)
chunk_limiter = trio.CapacityLimiter(MAX_CONCURRENT_CHUNK_BUILDERS)
embed_limiter = trio.CapacityLimiter(MAX_CONCURRENT_CHUNK_BUILDERS)
minio_limiter = trio.CapacityLimiter(MAX_CONCURRENT_MINIO)

# Redis Stream Consumer Group
CONSUMER_GROUP_NAME = "task_executor"
CONSUMER_NAME = f"task_executor_{CONSUMER_NO}"
```

**ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜**: `do_handle_task()` ([rag/svr/task_executor.py:759-890](rag/svr/task_executor.py#L759-L890))

```python
async def do_handle_task(task):
    task_id = task["id"]
    task_doc_id = task["doc_id"]
    task_kb_id = task["kb_id"]
    task_parser_config = task["parser_config"]
    task_embedding_id = task["embd_id"]

    progress_callback = partial(set_progress, task_id, ...)

    # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
    embedding_model = LLMBundle(
        tenant_id,
        LLMType.EMBEDDING,
        llm_name=task_embedding_id
    )

    # 2. ì²­í‚¹ (ë¬¸ì„œ â†’ ì²­í¬)
    chunks = await build_chunks(task, progress_callback)

    # 3. ì„ë² ë”© ìƒì„±
    tk_count, vector_size = await embedding(
        chunks,
        embedding_model,
        task_parser_config,
        progress_callback
    )

    # 4. Elasticsearch ì¸ë±ì‹±
    await insert_es(task_id, tenant_id, task_kb_id, chunks, progress_callback)

    # 5. ë¬¸ì„œ í†µê³„ ì—…ë°ì´íŠ¸
    DocumentService.update_by_id(task_doc_id, {
        "chunk_num": len(chunks),
        "token_num": tk_count,
        "progress": 1.0,
        "run": TaskStatus.DONE
    })

    # 6. KB í†µê³„ ì—…ë°ì´íŠ¸
    KnowledgebaseService.update_by_id(task_kb_id, {
        "chunk_num": kb.chunk_num + len(chunks),
        "token_num": kb.token_num + tk_count
    })
```

---

## 4. ì²­í‚¹(Chunking) ì „ëµ & í…ìŠ¤íŠ¸ ì¶”ì¶œ

### 4.1 íŒŒì„œ íŒ©í† ë¦¬ ì‹œìŠ¤í…œ

**íŒŒì„œ íƒ€ì…ë³„ êµ¬í˜„** ([rag/svr/task_executor.py:74-91](rag/svr/task_executor.py#L74-L91)):

```python
FACTORY = {
    ParserType.NAIVE.value: naive,       # ì¼ë°˜ í…ìŠ¤íŠ¸
    ParserType.QA.value: qa,             # Q&A ìŒ
    ParserType.PAPER.value: paper,       # ë…¼ë¬¸
    ParserType.BOOK.value: book,         # ì±…
    ParserType.PRESENTATION.value: presentation,  # PPT
    ParserType.MANUAL.value: manual,     # ë§¤ë‰´ì–¼
    ParserType.LAWS.value: laws,         # ë²•ë¥  ë¬¸ì„œ
    ParserType.TABLE.value: table,       # í‘œ ì¤‘ì‹¬
    ParserType.RESUME.value: resume,     # ì´ë ¥ì„œ
    ParserType.PICTURE.value: picture,   # ì´ë¯¸ì§€
    ParserType.ONE.value: one,           # ë‹¨ì¼ ì²­í¬
    ParserType.AUDIO.value: audio,       # ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸
    ParserType.EMAIL.value: email,       # ì´ë©”ì¼
    ParserType.TAG.value: tag            # íƒœê·¸ ì¶”ì¶œ
}
```

### 4.2 ì²­í‚¹ í”„ë¡œì„¸ìŠ¤

**í•¨ìˆ˜**: `build_chunks()` ([rag/svr/task_executor.py:262-381](rag/svr/task_executor.py#L262-L381))

```python
async def build_chunks(task, progress_callback):
    # 1. íŒŒì¼ í¬ê¸° ê²€ì¦
    if task["size"] > DOC_MAXIMUM_SIZE:
        raise Exception("File too large")

    # 2. íŒŒì„œ ì„ íƒ
    chunker = FACTORY[task["parser_id"].lower()]

    # 3. MinIOì—ì„œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
    bucket, name = File2DocumentService.get_storage_address(doc_id=task["doc_id"])
    binary = await get_storage_binary(bucket, name)

    # 4. ì²­í‚¹ ì‹¤í–‰ (íŒŒì„œë³„ ë¡œì§)
    async with chunk_limiter:
        cks = await trio.to_thread.run_sync(
            lambda: chunker.chunk(
                task["name"],
                binary=binary,
                from_page=task["from_page"],
                to_page=task["to_page"],
                lang=task["language"],
                callback=progress_callback,
                kb_id=task["kb_id"],
                parser_config=task["parser_config"],
                tenant_id=task["tenant_id"]
            )
        )

    # 5. ì²­í¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    docs = []
    for ck in cks:
        d = {
            "doc_id": task["doc_id"],
            "kb_id": str(task["kb_id"]),
            "content_with_weight": ck["content_with_weight"],
            "image": ck.get("image"),  # ì´ë¯¸ì§€ ì²­í¬
            ...ck  # ê¸°íƒ€ ë©”íƒ€ë°ì´í„°
        }

        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ MinIOì— ì €ì¥
        if d.get("image"):
            await image2id(d, STORAGE_IMPL.put, d["id"], task["kb_id"])

        docs.append(d)

    # 6. LLM ê¸°ë°˜ ì¦ê°• (ì„ íƒì )
    if task["parser_config"].get("auto_keywords", 0):
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        for d in docs:
            keywords = await keyword_extraction(chat_mdl, d["content_with_weight"], topn)
            d["important_kwd"] = keywords.split(",")

    if task["parser_config"].get("auto_questions", 0):
        # ì§ˆë¬¸ ìƒì„±
        for d in docs:
            questions = await question_proposal(chat_mdl, d["content_with_weight"], topn)
            d["question_kwd"] = questions.split("\n")

    return docs
```

### 4.3 íŒŒì„œë³„ ì²­í‚¹ ì „ëµ

#### **QA íŒŒì„œ** ([rag/app/qa.py:313](rag/app/qa.py#L313))

**ëª©ì **: Q&A ìŒ ì¶”ì¶œ (Excel, DOCX, PDF)

**ì²­í‚¹ ë¡œì§**:
```python
def chunk(filename, binary, from_page, to_page, lang, callback, **kwargs):
    # 1. íŒŒì¼ íƒ€ì…ë³„ íŒŒì„œ ì„ íƒ
    if filename.lower().endswith(".xlsx"):
        excel_parser = Excel()
        res = excel_parser(filename, binary, callback)  # [(Q, A), ...]

    elif filename.lower().endswith(".docx"):
        docx_parser = Docx()
        res = docx_parser(filename, binary)

    elif filename.lower().endswith(".pdf"):
        pdf_parser = Pdf()
        res = pdf_parser(filename, binary, from_page, to_page, callback)

    # 2. Q&A ìŒì„ ì²­í¬ë¡œ ë³€í™˜
    chunks = []
    for question, answer in res:
        chunk = {
            "content_with_weight": f"Question: {question}\nAnswer: {answer}",
            "content_ltks": rag_tokenizer.tokenize(question + " " + answer),
            "q": question,
            "a": answer
        }
        chunks.append(chunk)

    return chunks
```

#### **Naive íŒŒì„œ** ([rag/app/naive.py:433](rag/app/naive.py#L433))

**ëª©ì **: ì¼ë°˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ (ê°€ì¥ ë²”ìš©ì )

**ì²­í‚¹ ë‹¨ê³„**:
```python
def chunk(filename, binary, from_page, to_page, lang, callback, parser_config, **kwargs):
    # 1. íŒŒì¼ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    if is_pdf(filename):
        # DeepDoc Vision íŒŒì„œ ì‚¬ìš©
        sections = pdf_parser(binary, from_page, to_page, callback)
    elif is_docx(filename):
        sections = docx_parser(binary)
    elif is_excel(filename):
        sections = excel_parser(binary)
    elif is_pptx(filename):
        sections = pptx_parser(binary)

    # 2. ì„¹ì…˜ì„ í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹
    chunk_token_num = parser_config.get("chunk_token_num", 512)
    delimiter = parser_config.get("delimiter", "\n")

    chunks = []
    current_chunk = ""
    current_tokens = 0

    for section in sections:
        section_tokens = num_tokens_from_string(section["text"])

        if current_tokens + section_tokens > chunk_token_num:
            # í˜„ì¬ ì²­í¬ ì™„ë£Œ
            chunks.append({
                "content_with_weight": current_chunk,
                "content_ltks": rag_tokenizer.tokenize(current_chunk),
                "page_num": section["page_num"],
                "top": section["layout_bbox"]["top"],
                "image": section.get("image")  # ìŠ¤í¬ë¦°ìƒ·
            })
            current_chunk = section["text"]
            current_tokens = section_tokens
        else:
            current_chunk += delimiter + section["text"]
            current_tokens += section_tokens

    # 3. ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append(...)

    return chunks
```

#### **Table íŒŒì„œ** ([rag/app/table.py:302](rag/app/table.py#L302))

**ëª©ì **: í‘œ ì¤‘ì‹¬ ë¬¸ì„œ (ë°ì´í„° ì •í™•ì„± ì¤‘ì‹œ)

**íŠ¹ì§•**:
- í‘œ êµ¬ì¡° ì¸ì‹ (DeepDoc Vision)
- í‘œë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- ê° í‘œë¥¼ ë…ë¦½ì ì¸ ì²­í¬ë¡œ ìƒì„±

```python
def chunk(filename, binary, lang, callback, **kwargs):
    # 1. PDFì—ì„œ í‘œ ì¶”ì¶œ (Vision ëª¨ë¸)
    tables = pdf_parser.extract_tables(binary, callback)

    chunks = []
    for table in tables:
        # 2. HTML í‘œ â†’ Markdown ë³€í™˜
        markdown_table = html_table_to_markdown(table["html"])

        # 3. ì²­í¬ ìƒì„± (í‘œ ë‹¨ìœ„)
        chunk = {
            "content_with_weight": markdown_table,
            "content_ltks": tokenize_table(markdown_table),
            "page_num": table["page_num"],
            "table_id": table["id"],
            "image": table["screenshot"]  # í‘œ ì´ë¯¸ì§€
        }
        chunks.append(chunk)

    return chunks
```

### 4.4 DeepDoc Vision íŒŒì„œ

**PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸** ([rag/app/qa.py:79-99](rag/app/qa.py#L79-L99)):

```python
class Pdf(PdfParser):
    def __call__(self, filename, binary, from_page, to_page, zoomin=3, callback):
        # 1. OCR (ê´‘í•™ ë¬¸ì ì¸ì‹)
        self.__images__(binary, zoomin, from_page, to_page, callback)
        callback(msg="OCR finished")

        # 2. ë ˆì´ì•„ì›ƒ ì¸ì‹ (10ê°€ì§€ ë ˆì´ì•„ì›ƒ íƒ€ì…)
        self._layouts_rec(zoomin, drop=False)
        callback(msg="Layout analysis finished")
        # ë ˆì´ì•„ì›ƒ íƒ€ì…: Text, Title, Figure, Caption, Table,
        #               Table Caption, Header, Footer, Reference, Equation

        # 3. í‘œ êµ¬ì¡° ì¸ì‹ (5ê°€ì§€ í‘œ ì»´í¬ë„ŒíŠ¸)
        self._table_transformer_job(zoomin)
        callback(msg="Table analysis finished")

        # 4. Q&A ìŒ ì¶”ì¶œ
        qa_pairs = self._extract_qa_pairs()

        return qa_pairs
```

**Vision ëª¨ë¸ í™œìš©**:
- **OCR**: í…ìŠ¤íŠ¸ ìœ„ì¹˜ + ë‚´ìš© ì¶”ì¶œ
- **Layout Recognition**: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (ì œëª©, ë³¸ë¬¸, í‘œ, ê·¸ë¦¼ ë“±)
- **Table Recognition**: í‘œ ì…€ êµ¬ì¡° ì¸ì‹

---

## 5. ì„ë² ë”© ìƒì„± & ì¸ë±ì‹±

### 5.1 ì„ë² ë”© ìƒì„± í”„ë¡œì„¸ìŠ¤

**í•¨ìˆ˜**: `embedding()` ([rag/svr/task_executor.py:462-511](rag/svr/task_executor.py#L462-L511))

```python
async def embedding(docs, mdl, parser_config, callback):
    # 1. í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª© + ë‚´ìš©)
    tts, cnts = [], []  # titles, contents
    for d in docs:
        tts.append(d.get("docnm_kwd", "Title"))

        # ìš°ì„ ìˆœìœ„: ì§ˆë¬¸ > ì›ë³¸ ë‚´ìš©
        c = "\n".join(d.get("question_kwd", []))
        if not c:
            c = d["content_with_weight"]

        # HTML í…Œì´ë¸” íƒœê·¸ ì œê±°
        c = re.sub(r"</?(table|td|caption|tr|th)>", " ", c)
        cnts.append(c)

    # 2. ì œëª© ì„ë² ë”© (ëª¨ë“  ì²­í¬ì— ë™ì¼í•œ ì œëª© ë²¡í„° ì‚¬ìš©)
    title_vecs, token_count = mdl.encode(tts[0:1])
    tts_embeds = np.repeat(title_vecs[0], len(tts), axis=0)

    # 3. ë‚´ìš© ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)
    content_embeds = []
    for i in range(0, len(cnts), EMBEDDING_BATCH_SIZE):
        batch = cnts[i:i+EMBEDDING_BATCH_SIZE]

        # ìµœëŒ€ ê¸¸ì´ ìë¥´ê¸°
        batch_truncated = [truncate(c, mdl.max_length-10) for c in batch]

        async with embed_limiter:
            vecs, count = await trio.to_thread.run_sync(
                lambda: mdl.encode(batch_truncated)
            )

        content_embeds.append(vecs)
        token_count += count
        callback(prog=0.7 + 0.2 * (i+1)/len(cnts), msg="")

    content_embeds = np.concatenate(content_embeds, axis=0)

    # 4. ê°€ì¤‘ í‰ê·  (ì œëª© + ë‚´ìš©)
    filename_embd_weight = parser_config.get("filename_embd_weight", 0.1)
    title_w = float(filename_embd_weight)

    final_vecs = (title_w * tts_embeds + (1 - title_w) * content_embeds)

    # 5. ë²¡í„°ë¥¼ ë¬¸ì„œì— ì¶”ê°€
    vector_size = final_vecs.shape[1]
    for i, d in enumerate(docs):
        d[f"q_{vector_size}_vec"] = final_vecs[i].tolist()

    return token_count, vector_size
```

**ì„ë² ë”© ëª¨ë¸ ì§€ì›**:
- **BAAI/bge-large-zh-v1.5** (ì¤‘êµ­ì–´)
- **BAAI/bge-large-en-v1.5** (ì˜ì–´)
- **OpenAI text-embedding-3-small/large**
- **Infinity SDK** (ë¡œì»¬ ë°°í¬)
- **Voyager AI**, **Cohere Embed v3**

### 5.2 Elasticsearch ì¸ë±ì‹±

**í•¨ìˆ˜**: `insert_es()` ([rag/svr/task_executor.py:731-756](rag/svr/task_executor.py#L731-L756))

```python
async def insert_es(task_id, tenant_id, kb_id, chunks, progress_callback):
    # ë°°ì¹˜ ì¸ì„œíŠ¸ (DOC_BULK_SIZE = 4)
    for b in range(0, len(chunks), DOC_BULK_SIZE):
        batch = chunks[b:b+DOC_BULK_SIZE]

        # Elasticsearch bulk insert
        doc_store_result = await trio.to_thread.run_sync(
            lambda: settings.docStoreConn.insert(
                batch,
                index_name=search.index_name(tenant_id),
                kb_id=kb_id
            )
        )

        if doc_store_result:
            # ì‹¤íŒ¨ ì‹œ ë¡¤ë°±
            progress_callback(-1, msg=f"Insert chunk error: {doc_store_result}")
            raise Exception(doc_store_result)

        # Taskì— chunk_ids ê¸°ë¡ (ë¡¤ë°±ìš©)
        chunk_ids = [chunk["id"] for chunk in batch]
        TaskService.update_chunk_ids(task_id, " ".join(chunk_ids))

        progress_callback(prog=0.8 + 0.1*(b+1)/len(chunks), msg="")

    return True
```

### 5.3 Elasticsearch ì¸ë±ìŠ¤ êµ¬ì¡°

**ì¸ë±ìŠ¤ ì´ë¦„**: `{tenant_id}_{kb_id}`

**í•„ë“œ ë§¤í•‘** (ì¶”ì •):
```json
{
  "mappings": {
    "properties": {
      "id": {"type": "keyword"},
      "doc_id": {"type": "keyword"},
      "kb_id": {"type": "keyword"},
      "docnm_kwd": {"type": "keyword"},
      "title_tks": {"type": "text", "analyzer": "ik_max_word"},
      "content_with_weight": {"type": "text"},
      "content_ltks": {"type": "text", "analyzer": "ik_max_word"},
      "content_sm_ltks": {"type": "text", "analyzer": "ik_smart"},
      "q_1024_vec": {"type": "dense_vector", "dims": 1024},
      "important_kwd": {"type": "keyword"},
      "question_kwd": {"type": "keyword"},
      "tag_kwd": {"type": "keyword"},
      "page_num": {"type": "integer"},
      "top": {"type": "float"},
      "img_id": {"type": "keyword"},
      "create_time": {"type": "date"},
      "pagerank": {"type": "integer"}
    }
  }
}
```

**ë²¡í„° í•„ë“œ ë™ì  ìƒì„±**:
- `q_768_vec` (BAAI/bge-large)
- `q_1024_vec` (OpenAI ada-002)
- `q_1536_vec` (OpenAI text-embedding-3-small)

---

## 6. í”„ë¡ íŠ¸ì—”ë“œ UI ì»´í¬ë„ŒíŠ¸

### 6.1 ë°ì´í„°ì…‹ ëª©ë¡ í˜ì´ì§€

**ì»´í¬ë„ŒíŠ¸**: [web/src/pages/datasets/index.tsx](web/src/pages/datasets/index.tsx)

**ì£¼ìš” ê¸°ëŠ¥**:
```tsx
export default function Datasets() {
  const { kbs, total, pagination, setPagination } = useFetchNextKnowledgeListByPage();
  const { visible, showModal, onCreateOk } = useSaveKnowledge();

  return (
    <section>
      {/* í•„í„° & ê²€ìƒ‰ ë°” */}
      <ListFilterBar
        title="Dataset"
        onSearchChange={handleInputChange}
        filters={owners}
      >
        <Button onClick={showModal}>Create Knowledge Base</Button>
      </ListFilterBar>

      {/* ë°ì´í„°ì…‹ ì¹´ë“œ ê·¸ë¦¬ë“œ */}
      <CardContainer>
        {kbs.map(dataset => (
          <DatasetCard
            dataset={dataset}
            key={dataset.id}
            showRenameModal={showDatasetRenameModal}
          />
        ))}
      </CardContainer>

      {/* í˜ì´ì§€ë„¤ì´ì…˜ */}
      <RAGFlowPagination
        total={total}
        pagination={pagination}
        onChange={handlePageChange}
      />

      {/* ìƒì„± ë‹¤ì´ì–¼ë¡œê·¸ */}
      <DatasetCreatingDialog
        visible={visible}
        onOk={onCreateOk}
        loading={creatingLoading}
      />
    </section>
  );
}
```

### 6.2 ë°ì´í„°ì…‹ ìƒì„± ë‹¤ì´ì–¼ë¡œê·¸

**ì»´í¬ë„ŒíŠ¸**: `DatasetCreatingDialog`

**ì…ë ¥ í•„ë“œ**:
```tsx
<Form>
  <FormField name="name" label="Dataset Name">
    <Input placeholder="My Knowledge Base" />
  </FormField>

  <FormField name="description" label="Description">
    <Textarea placeholder="Optional description" />
  </FormField>

  <FormField name="parser_id" label="Parser Type">
    <Select>
      <option value="naive">General</option>
      <option value="qa">Q&A</option>
      <option value="manual">Manual</option>
      <option value="paper">Academic Paper</option>
      <option value="book">Book</option>
      <option value="laws">Legal Document</option>
      <option value="presentation">Presentation</option>
      <option value="table">Table</option>
      <option value="resume">Resume</option>
      <option value="picture">Image</option>
      <option value="one">Single Chunk</option>
      <option value="audio">Audio</option>
      <option value="email">Email</option>
    </Select>
  </FormField>

  <FormField name="embd_id" label="Embedding Model">
    <Select>
      <option value="BAAI/bge-large-zh-v1.5">BAAI BGE Large (Chinese)</option>
      <option value="BAAI/bge-large-en-v1.5">BAAI BGE Large (English)</option>
      <option value="text-embedding-3-small">OpenAI ada-002</option>
    </Select>
  </FormField>
</Form>
```

### 6.3 ë°ì´í„°ì…‹ ìƒì„¸ í˜ì´ì§€

**ë¼ìš°íŠ¸**: `/datasets/{kb_id}`

**ì£¼ìš” ì„¹ì…˜**:
```tsx
<DatasetDetail>
  {/* í—¤ë” */}
  <Header>
    <Title>{kb.name}</Title>
    <Stats>
      <Stat label="Documents" value={kb.doc_num} />
      <Stat label="Chunks" value={kb.chunk_num} />
      <Stat label="Tokens" value={kb.token_num} />
      <Stat label="Size" value={formatBytes(kb.size)} />
    </Stats>
  </Header>

  {/* íƒ­ ë„¤ë¹„ê²Œì´ì…˜ */}
  <Tabs>
    <Tab label="Documents" />
    <Tab label="Configuration" />
    <Tab label="Graph" />
    <Tab label="Logs" />
  </Tabs>

  {/* ë¬¸ì„œ ëª©ë¡ íƒ­ */}
  <DocumentList>
    <UploadButton onClick={handleUpload}>Upload Files</UploadButton>
    <DocumentTable
      documents={documents}
      onDelete={handleDelete}
      onReparse={handleReparse}
    />
  </DocumentList>

  {/* ì„¤ì • íƒ­ */}
  <ConfigurationPanel>
    <ParserConfig config={kb.parser_config} onChange={handleConfigChange} />
    <EmbeddingModelSelector value={kb.embd_id} onChange={handleModelChange} />
  </ConfigurationPanel>

  {/* ì§€ì‹ ê·¸ë˜í”„ íƒ­ */}
  <KnowledgeGraph kb_id={kb.id} />
</DatasetDetail>
```

### 6.4 ë¬¸ì„œ ì—…ë¡œë“œ ì»´í¬ë„ŒíŠ¸

```tsx
<Dropzone onDrop={handleFileDrop}>
  <input type="file" multiple accept=".pdf,.docx,.xlsx,.pptx,.txt,.md" />
  <UploadIcon />
  <p>Drag and drop files here, or click to select files</p>
  <p className="text-sm text-muted">Supported formats: PDF, DOCX, XLSX, PPTX, TXT, MD</p>
</Dropzone>

{/* ì—…ë¡œë“œ ì§„í–‰ë¥  */}
{uploadingFiles.map(file => (
  <UploadProgress
    key={file.id}
    filename={file.name}
    progress={file.progress}
    status={file.status}  // uploading|parsing|done|failed
  />
))}
```

---

## 7. ì „ì²´ ì›Œí¬í”Œë¡œìš° ìš”ì•½

### 7.1 ì™„ì „í•œ ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ì§€ì‹ë² ì´ìŠ¤ ìƒì„± (Frontend)                                     â”‚
â”‚    - ì´ë¦„, ì„¤ëª…, íŒŒì„œ íƒ€ì…, ì„ë² ë”© ëª¨ë¸ ì„ íƒ                        â”‚
â”‚    - POST /v1/dataset/create                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Knowledgebase ë ˆì½”ë“œ ìƒì„± (Backend)                            â”‚
â”‚    - UUID ìƒì„±, parser_config ì´ˆê¸°í™”                              â”‚
â”‚    - MySQLì— ì €ì¥                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. íŒŒì¼ ì—…ë¡œë“œ (Frontend â†’ Backend)                               â”‚
â”‚    - POST /v1/document/upload (multipart/form-data)              â”‚
â”‚    - ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì—…ë¡œë“œ ê°€ëŠ¥                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. íŒŒì¼ ì €ì¥ (Backend)                                            â”‚
â”‚    â”œâ”€ MinIOì— ë°”ì´ë„ˆë¦¬ ì €ì¥ (bucket: kb_id, key: file_uuid)      â”‚
â”‚    â”œâ”€ File ë ˆì½”ë“œ ìƒì„± (parent_id, location, size)                â”‚
â”‚    â”œâ”€ Document ë ˆì½”ë“œ ìƒì„± (kb_id, parser_id, run=UNSTART)        â”‚
â”‚    â””â”€ File2Document ê´€ê³„ ìƒì„±                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. íŒŒì‹± íƒœìŠ¤í¬ íì‰ (Backend)                                      â”‚
â”‚    - Task ë ˆì½”ë“œ ìƒì„± (task_type="dataflow")                      â”‚
â”‚    - Redis Streamì— ì¶”ê°€ (queue_name)                             â”‚
â”‚    - Document.run = RUNNING                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Workerê°€ íƒœìŠ¤í¬ ì²˜ë¦¬ (task_executor.py)                        â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ 6.1 MinIOì—ì„œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°                        â”‚          â”‚
â”‚    â”‚     â†’ get_storage_binary(bucket, name)            â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                   â†“                                               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ 6.2 ì²­í‚¹ (build_chunks)                            â”‚          â”‚
â”‚    â”‚     â”œâ”€ íŒŒì„œ ì„ íƒ (FACTORY[parser_id])              â”‚          â”‚
â”‚    â”‚     â”œâ”€ Vision íŒŒì‹± (PDF: OCR + Layout + Table)    â”‚          â”‚
â”‚    â”‚     â”œâ”€ í…ìŠ¤íŠ¸ ì¶”ì¶œ & ë¶„í•  (í† í° ìˆ˜ ê¸°ì¤€)            â”‚          â”‚
â”‚    â”‚     â”œâ”€ LLM ì¦ê°• (auto_keywords, auto_questions)   â”‚          â”‚
â”‚    â”‚     â””â”€ ì´ë¯¸ì§€ ì²­í¬ MinIO ì €ì¥                       â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                   â†“                                               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ 6.3 ì„ë² ë”© ìƒì„± (embedding)                        â”‚          â”‚
â”‚    â”‚     â”œâ”€ ì„ë² ë”© ëª¨ë¸ ë¡œë”© (LLMBundle)                 â”‚          â”‚
â”‚    â”‚     â”œâ”€ ì œëª© + ë‚´ìš© ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)                â”‚          â”‚
â”‚    â”‚     â”œâ”€ ê°€ì¤‘ í‰ê·  (filename_embd_weight=0.1)        â”‚          â”‚
â”‚    â”‚     â””â”€ ë²¡í„°ë¥¼ ì²­í¬ì— ì¶”ê°€ (q_{size}_vec)            â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                   â†“                                               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ 6.4 Elasticsearch ì¸ë±ì‹± (insert_es)               â”‚          â”‚
â”‚    â”‚     â”œâ”€ ë°°ì¹˜ ì¸ì„œíŠ¸ (DOC_BULK_SIZE=4)               â”‚          â”‚
â”‚    â”‚     â”œâ”€ ì¸ë±ìŠ¤: {tenant_id}_{kb_id}                â”‚          â”‚
â”‚    â”‚     â”œâ”€ ë²¡í„° í•„ë“œ, ì „ë¬¸ ê²€ìƒ‰ í•„ë“œ, ë©”íƒ€ë°ì´í„°          â”‚          â”‚
â”‚    â”‚     â””â”€ chunk_ids ê¸°ë¡ (ë¡¤ë°±ìš©)                      â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                   â†“                                               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ 6.5 í†µê³„ ì—…ë°ì´íŠ¸                                   â”‚          â”‚
â”‚    â”‚     â”œâ”€ Document (chunk_num, token_num, run=DONE)  â”‚          â”‚
â”‚    â”‚     â””â”€ Knowledgebase (chunk_numâ†‘, token_numâ†‘)     â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€ï¿½ï¿½ï¿½â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. ê³ ê¸‰ ê¸°ëŠ¥ ì‹¤í–‰ (ì„ íƒì )                                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ 7.1 GraphRAG (POST /v1/dataset/run_graphrag)      â”‚          â”‚
â”‚    â”‚     â”œâ”€ ì—”í‹°í‹° ì¶”ì¶œ (organization, person, geo...)  â”‚          â”‚
â”‚    â”‚     â”œâ”€ ê´€ê³„ ì¶”ì¶œ (ì—”í‹°í‹° ê°„ ì—°ê²°)                    â”‚          â”‚
â”‚    â”‚     â””â”€ ì§€ì‹ ê·¸ë˜í”„ Elasticsearch ì €ì¥                â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ 7.2 RAPTOR (POST /v1/dataset/run_raptor)          â”‚          â”‚
â”‚    â”‚     â”œâ”€ ì²­í¬ í´ëŸ¬ìŠ¤í„°ë§ (ê³„ì¸µì )                      â”‚          â”‚
â”‚    â”‚     â”œâ”€ LLM ìš”ì•½ (ê° í´ëŸ¬ìŠ¤í„°)                        â”‚          â”‚
â”‚    â”‚     â””â”€ ìš”ì•½ ì²­í¬ ì¸ë±ì‹± (íŠ¸ë¦¬ êµ¬ì¡°)                  â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. ì§€ì‹ë² ì´ìŠ¤ ì‚¬ìš© ê°€ëŠ¥                                             â”‚
â”‚    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)                                 â”‚
â”‚    - ì¬ìˆœìœ„í™” (Reranking)                                          â”‚
â”‚    - RAG ê¸°ë°˜ ì±„íŒ…                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 ë°ì´í„° í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨

```
User (Frontend)
    â†“ [íŒŒì¼ ì—…ë¡œë“œ]
MinIO (Object Storage)
    â†“ [ë°”ì´ë„ˆë¦¬ ì €ì¥]
MySQL (Metadata DB)
    â†“ [File, Document, Task ë ˆì½”ë“œ]
Redis Stream (Task Queue)
    â†“ [íƒœìŠ¤í¬ íì‰]
task_executor.py (Worker)
    â”œâ”€â†’ [1] MinIOì—ì„œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
    â”œâ”€â†’ [2] Vision íŒŒì„œ (OCR, Layout, Table)
    â”œâ”€â†’ [3] ì²­í‚¹ (íŒŒì„œë³„ ë¡œì§)
    â”œâ”€â†’ [4] LLM ì¦ê°• (í‚¤ì›Œë“œ, ì§ˆë¬¸)
    â”œâ”€â†’ [5] ì„ë² ë”© ìƒì„± (ë°°ì¹˜)
    â””â”€â†’ [6] Elasticsearch ì¸ë±ì‹±
         â†“
Elasticsearch (Vector + Full-text Index)
    â†“ [ê²€ìƒ‰ ê°€ëŠ¥]
RAG Pipeline (ê²€ìƒ‰ â†’ ì¬ìˆœìœ„í™” â†’ LLM ìƒì„±)
```

### 7.3 ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ

| ë‹¨ê³„ | ì†Œìš” ì‹œê°„ (ì˜ˆìƒ) | ë³‘ëª© ìš”ì¸ |
|------|------------------|----------|
| íŒŒì¼ ì—…ë¡œë“œ | 1-10ì´ˆ | ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ |
| MinIO ì €ì¥ | <1ì´ˆ | ìŠ¤í† ë¦¬ì§€ I/O |
| Vision íŒŒì‹± (PDF) | 2-30ì´ˆ/í˜ì´ì§€ | GPU ê°€ìš©ì„±, ì´ë¯¸ì§€ í•´ìƒë„ |
| ì²­í‚¹ | 1-5ì´ˆ/ë¬¸ì„œ | í…ìŠ¤íŠ¸ ë³µì¡ë„ |
| LLM ì¦ê°• | 10-60ì´ˆ/ì²­í¬ | LLM API ë ˆì´íŠ¸ ë¦¬ë°‹ |
| ì„ë² ë”© ìƒì„± | 0.1-1ì´ˆ/ë°°ì¹˜(16ì²­í¬) | GPU/ì„ë² ë”© ëª¨ë¸ |
| Elasticsearch ì¸ë±ì‹± | 0.5-2ì´ˆ/ë°°ì¹˜(4ì²­í¬) | ES í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ |

**ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ì˜ˆì‹œ**:
- 10í˜ì´ì§€ PDF â†’ ì•½ 50ê°œ ì²­í¬ â†’ 2-5ë¶„ (Vision íŒŒì‹± í¬í•¨)
- 100í˜ì´ì§€ PDF â†’ ì•½ 500ê°œ ì²­í¬ â†’ 20-50ë¶„
- 1000ê°œ DOCX â†’ ë³‘ë ¬ ì²˜ë¦¬ë¡œ 3-10ì‹œê°„

### 7.4 í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

**ë™ì‹œ ì²˜ë¦¬**:

```python
MAX_CONCURRENT_TASKS = 5          # ë™ì‹œ íƒœìŠ¤í¬
MAX_CONCURRENT_CHUNK_BUILDERS = 1 # ì²­í‚¹ ì‘ì—… (CPU ì§‘ì•½)
MAX_CONCURRENT_MINIO = 10         # MinIO I/O
EMBEDDING_BATCH_SIZE = 16         # ì„ë² ë”© ë°°ì¹˜
DOC_BULK_SIZE = 4                 # Elasticsearch ë°°ì¹˜
```

**ìŠ¤ì¼€ì¼ë§ ì „ëµ**:

1. **ìˆ˜í‰ í™•ì¥**: Worker ì¸ìŠ¤í„´ìŠ¤ ì¦ê°€
2. **GPU í™œìš©**: Vision íŒŒì‹± & ì„ë² ë”© ê°€ì†
3. **ES ìƒ¤ë”©**: ì¸ë±ìŠ¤ ë¶„ì‚°
4. **ìºì‹±**: LLM í˜¸ì¶œ ê²°ê³¼ Redis ìºì‹±

---

## 8. í•µì‹¬ íŒŒì¼ ì°¸ì¡° ì¸ë±ìŠ¤

| ê¸°ëŠ¥ | íŒŒì¼ ê²½ë¡œ |
|------|----------|
| **ë°ì´í„° ëª¨ë¸** | [api/db/db_models.py:736-815](api/db/db_models.py#L736-L815) |
| **KB API** | [api/apps/kb_app.py](api/apps/kb_app.py) |
| **ë¬¸ì„œ API** | [api/apps/document_app.py](api/apps/document_app.py) |
| **íŒŒì¼ ì„œë¹„ìŠ¤** | [api/db/services/file_service.py](api/db/services/file_service.py) |
| **KB ì„œë¹„ìŠ¤** | [api/db/services/knowledgebase_service.py](api/db/services/knowledgebase_service.py) |
| **ë¬¸ì„œ ì„œë¹„ìŠ¤** | [api/db/services/document_service.py](api/db/services/document_service.py) |
| **íƒœìŠ¤í¬ ì‹¤í–‰ì** | [rag/svr/task_executor.py](rag/svr/task_executor.py) |
| **QA íŒŒì„œ** | [rag/app/qa.py](rag/app/qa.py) |
| **Naive íŒŒì„œ** | [rag/app/naive.py](rag/app/naive.py) |
| **Table íŒŒì„œ** | [rag/app/table.py](rag/app/table.py) |
| **í”„ë¡ íŠ¸ì—”ë“œ** | [web/src/pages/datasets/](web/src/pages/datasets/) |

---

## ìš”ì•½

RAGFlowì˜ ë°ì´í„°ì…‹(ì§€ì‹ë² ì´ìŠ¤) êµ¬ì¶• ì‹œìŠ¤í…œì€ **ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤:

âœ… **ë©€í‹° íŒŒì„œ ì§€ì›**: 13ê°œ ë¬¸ì„œ íƒ€ì…ë³„ ìµœì í™”ëœ íŒŒì‹± ì „ëµ
âœ… **Vision AI í†µí•©**: DeepDocìœ¼ë¡œ OCR, ë ˆì´ì•„ì›ƒ, í‘œ êµ¬ì¡° ìë™ ì¸ì‹
âœ… **ì§€ëŠ¥í˜• ì²­í‚¹**: í† í° ìˆ˜ ê¸°ë°˜ + ë¬¸ì„œ êµ¬ì¡° ê³ ë ¤
âœ… **LLM ì¦ê°•**: ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ, ì§ˆë¬¸ ìƒì„±, íƒœê¹…
âœ… **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ë²¡í„° ìœ ì‚¬ë„ + ì „ë¬¸ ê²€ìƒ‰ ê²°í•©
âœ… **í™•ì¥ì„±**: Worker ìˆ˜í‰ í™•ì¥, GPU ê°€ì†, ë¹„ë™ê¸° ì²˜ë¦¬
âœ… **ê³ ê¸‰ ê¸°ëŠ¥**: GraphRAG, RAPTOR, Mindmap ì§€ì›

ì´ ì‹œìŠ¤í…œì€ **ëŒ€ê·œëª¨ ë¬¸ì„œ ì»¬ë ‰ì…˜**ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , **ê³ í’ˆì§ˆì˜ ê²€ìƒ‰ ê²°ê³¼**ë¥¼ ì œê³µí•˜ì—¬ RAG ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
